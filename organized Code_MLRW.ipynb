{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","Name- Pranav Kushare\n","Kaggle UserName- Heisenberog_08\n","Email- pranav.kushare2001@gmail.com\n","\n","Approach-\n","Aim of the hackathon was to classify the give gene expression signature in category control or perturb. I did a lot of experimentations, but its not possible to document everything so in this writeup I will try to explain the approach that gave me the best score.\n","\n","1. The most crucial task was the appropriate feature/column selection out of those 54 columns. And then curating more data from the unlabelled data.\n","   After a lot of hit and trial I was able to choose [\"characteristics_ch1\",\"molecule_ch1\",\"organism_ch1\",'type',\"title\"] this columns which was relevant for our classification task. \n","\n","2. I tried going for simple approach that Is using only small labelled dataset for training and making submission. \n","   As the data was very less it was not well generalized for testing data. That lead to the need of generating more data.\n","\n","3.Next step was to generate more data. Larger datasets helped model to become more robust. So we had around 20k unlabelled datapoints. \n","  I trained a voting ensemble classifier with 7 models- Naïve bayes, RF, DT, Adaboost, gradient boosting, xgb, mlp.   And used soft voting method (weight average of probabilities) to predict the labels for 15,000 samples.\n","\n","4. So Now data size has been increased to 15,000 samples. I trained the classification model on this extended data and at the end logistic regression gave me the best accuracy.\n","\n","\n","\n","'''"]},{"cell_type":"code","execution_count":168,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-03-02T08:39:27.758604Z","iopub.status.busy":"2022-03-02T08:39:27.757632Z","iopub.status.idle":"2022-03-02T08:39:27.775614Z","shell.execute_reply":"2022-03-02T08:39:27.774898Z","shell.execute_reply.started":"2022-03-02T08:39:27.758547Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/mlrw-biomedicalhackathon/sample_submission.csv\n","/kaggle/input/mlrw-biomedicalhackathon/data_only_test.csv\n","/kaggle/input/mlrw-biomedicalhackathon/unlabelled_train_data.csv\n","/kaggle/input/mlrw-biomedicalhackathon/labelled_train_data.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["## Importing required libraries"]},{"cell_type":"code","execution_count":169,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:39:27.778065Z","iopub.status.busy":"2022-03-02T08:39:27.777176Z","iopub.status.idle":"2022-03-02T08:39:27.788691Z","shell.execute_reply":"2022-03-02T08:39:27.787776Z","shell.execute_reply.started":"2022-03-02T08:39:27.778017Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["# Used in all sections for managing data and files\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","import pandas as pd\n","import pickle\n","import re\n","\n","# NTLK is used for preprocessing text. You can find out more about each module using their documentation.\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","from nltk.corpus import wordnet as wn\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk import word_tokenize, pos_tag\n","from nltk.corpus import inaugural, stopwords\n","from wordcloud import WordCloud, STOPWORDS\n","\n","# Scikit-Learn is used for feature extraction and training a logistic regression model\n","from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n"]},{"cell_type":"code","execution_count":170,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:39:27.790296Z","iopub.status.busy":"2022-03-02T08:39:27.789852Z","iopub.status.idle":"2022-03-02T08:39:27.826244Z","shell.execute_reply":"2022-03-02T08:39:27.825370Z","shell.execute_reply.started":"2022-03-02T08:39:27.790264Z"},"trusted":true},"outputs":[],"source":["labelled_training_data_path = '../input/mlrw-biomedicalhackathon/labelled_train_data.csv'\n","train_df = pd.read_csv(labelled_training_data_path)"]},{"cell_type":"markdown","metadata":{},"source":["## Exploring the data"]},{"cell_type":"code","execution_count":171,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:39:27.828423Z","iopub.status.busy":"2022-03-02T08:39:27.828197Z","iopub.status.idle":"2022-03-02T08:39:27.866522Z","shell.execute_reply":"2022-03-02T08:39:27.865711Z","shell.execute_reply.started":"2022-03-02T08:39:27.828396Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>geo_accession</th>\n","      <th>gse_id</th>\n","      <th>ctrl</th>\n","      <th>pert</th>\n","      <th>channel_count</th>\n","      <th>characteristics_ch1</th>\n","      <th>contact_address</th>\n","      <th>contact_city</th>\n","      <th>contact_country</th>\n","      <th>...</th>\n","      <th>extract_protocol_ch2</th>\n","      <th>label_ch2</th>\n","      <th>label_protocol_ch2</th>\n","      <th>molecule_ch2</th>\n","      <th>organism_ch2</th>\n","      <th>source_name_ch2</th>\n","      <th>taxid_ch2</th>\n","      <th>treatment_protocol_ch2</th>\n","      <th>biomaterial_provider_ch2</th>\n","      <th>growth_protocol_ch2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>GSM1617977</td>\n","      <td>GSE66250</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>facs sorting: CD44low/CD24high</td>\n","      <td>Am Hubland</td>\n","      <td>Wuerzburg</td>\n","      <td>Germany</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>GSM1617983</td>\n","      <td>GSE66250</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>facs sorting: Unsorted</td>\n","      <td>Am Hubland</td>\n","      <td>Wuerzburg</td>\n","      <td>Germany</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>GSM1617982</td>\n","      <td>GSE66250</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>facs sorting: CD44low/CD24high</td>\n","      <td>Am Hubland</td>\n","      <td>Wuerzburg</td>\n","      <td>Germany</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>GSM1617975</td>\n","      <td>GSE66250</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>facs sorting: CD44high/CD24low</td>\n","      <td>Am Hubland</td>\n","      <td>Wuerzburg</td>\n","      <td>Germany</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>GSM1267968</td>\n","      <td>GSE52505</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>tissue: human nasal polyp</td>\n","      <td>148, Gurodong-ro, Guro-gu</td>\n","      <td>Seoul</td>\n","      <td>South Korea</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>618</th>\n","      <td>0</td>\n","      <td>GSM1462977</td>\n","      <td>GSE59980</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>cell line: MCF7</td>\n","      <td>1450 Biggy Street NRT6514</td>\n","      <td>Los Angeles</td>\n","      <td>USA</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>619</th>\n","      <td>1</td>\n","      <td>GSM1462972</td>\n","      <td>GSE59980</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>cell line: MCF7</td>\n","      <td>1450 Biggy Street NRT6514</td>\n","      <td>Los Angeles</td>\n","      <td>USA</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>620</th>\n","      <td>2</td>\n","      <td>GSM1462974</td>\n","      <td>GSE59980</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>cell line: MCF7</td>\n","      <td>1450 Biggy Street NRT6514</td>\n","      <td>Los Angeles</td>\n","      <td>USA</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>621</th>\n","      <td>3</td>\n","      <td>GSM1462976</td>\n","      <td>GSE59980</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>cell line: MCF7</td>\n","      <td>1450 Biggy Street NRT6514</td>\n","      <td>Los Angeles</td>\n","      <td>USA</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>622</th>\n","      <td>4</td>\n","      <td>GSM1462973</td>\n","      <td>GSE59980</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>cell line: MCF7</td>\n","      <td>1450 Biggy Street NRT6514</td>\n","      <td>Los Angeles</td>\n","      <td>USA</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>623 rows × 53 columns</p>\n","</div>"],"text/plain":["     Unnamed: 0 geo_accession    gse_id  ctrl  pert  channel_count  \\\n","0             0    GSM1617977  GSE66250   0.0   1.0              1   \n","1             1    GSM1617983  GSE66250   0.0   1.0              1   \n","2             2    GSM1617982  GSE66250   1.0   0.0              1   \n","3             3    GSM1617975  GSE66250   0.0   1.0              1   \n","4             0    GSM1267968  GSE52505   0.0   1.0              1   \n","..          ...           ...       ...   ...   ...            ...   \n","618           0    GSM1462977  GSE59980   0.0   1.0              1   \n","619           1    GSM1462972  GSE59980   1.0   0.0              1   \n","620           2    GSM1462974  GSE59980   1.0   0.0              1   \n","621           3    GSM1462976  GSE59980   0.0   1.0              1   \n","622           4    GSM1462973  GSE59980   1.0   0.0              1   \n","\n","                characteristics_ch1            contact_address contact_city  \\\n","0    facs sorting: CD44low/CD24high                 Am Hubland    Wuerzburg   \n","1            facs sorting: Unsorted                 Am Hubland    Wuerzburg   \n","2    facs sorting: CD44low/CD24high                 Am Hubland    Wuerzburg   \n","3    facs sorting: CD44high/CD24low                 Am Hubland    Wuerzburg   \n","4         tissue: human nasal polyp  148, Gurodong-ro, Guro-gu        Seoul   \n","..                              ...                        ...          ...   \n","618                 cell line: MCF7  1450 Biggy Street NRT6514  Los Angeles   \n","619                 cell line: MCF7  1450 Biggy Street NRT6514  Los Angeles   \n","620                 cell line: MCF7  1450 Biggy Street NRT6514  Los Angeles   \n","621                 cell line: MCF7  1450 Biggy Street NRT6514  Los Angeles   \n","622                 cell line: MCF7  1450 Biggy Street NRT6514  Los Angeles   \n","\n","    contact_country  ... extract_protocol_ch2 label_ch2 label_protocol_ch2  \\\n","0           Germany  ...                  NaN       NaN                NaN   \n","1           Germany  ...                  NaN       NaN                NaN   \n","2           Germany  ...                  NaN       NaN                NaN   \n","3           Germany  ...                  NaN       NaN                NaN   \n","4       South Korea  ...                  NaN       NaN                NaN   \n","..              ...  ...                  ...       ...                ...   \n","618             USA  ...                  NaN       NaN                NaN   \n","619             USA  ...                  NaN       NaN                NaN   \n","620             USA  ...                  NaN       NaN                NaN   \n","621             USA  ...                  NaN       NaN                NaN   \n","622             USA  ...                  NaN       NaN                NaN   \n","\n","    molecule_ch2 organism_ch2 source_name_ch2  taxid_ch2  \\\n","0            NaN          NaN             NaN        NaN   \n","1            NaN          NaN             NaN        NaN   \n","2            NaN          NaN             NaN        NaN   \n","3            NaN          NaN             NaN        NaN   \n","4            NaN          NaN             NaN        NaN   \n","..           ...          ...             ...        ...   \n","618          NaN          NaN             NaN        NaN   \n","619          NaN          NaN             NaN        NaN   \n","620          NaN          NaN             NaN        NaN   \n","621          NaN          NaN             NaN        NaN   \n","622          NaN          NaN             NaN        NaN   \n","\n","    treatment_protocol_ch2 biomaterial_provider_ch2 growth_protocol_ch2  \n","0                      NaN                      NaN                 NaN  \n","1                      NaN                      NaN                 NaN  \n","2                      NaN                      NaN                 NaN  \n","3                      NaN                      NaN                 NaN  \n","4                      NaN                      NaN                 NaN  \n","..                     ...                      ...                 ...  \n","618                    NaN                      NaN                 NaN  \n","619                    NaN                      NaN                 NaN  \n","620                    NaN                      NaN                 NaN  \n","621                    NaN                      NaN                 NaN  \n","622                    NaN                      NaN                 NaN  \n","\n","[623 rows x 53 columns]"]},"execution_count":171,"metadata":{},"output_type":"execute_result"}],"source":["train_df"]},{"cell_type":"code","execution_count":172,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:39:27.868003Z","iopub.status.busy":"2022-03-02T08:39:27.867663Z","iopub.status.idle":"2022-03-02T08:39:27.892810Z","shell.execute_reply":"2022-03-02T08:39:27.892078Z","shell.execute_reply.started":"2022-03-02T08:39:27.867955Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 623 entries, 0 to 622\n","Data columns (total 53 columns):\n"," #   Column                    Non-Null Count  Dtype  \n","---  ------                    --------------  -----  \n"," 0   Unnamed: 0                623 non-null    int64  \n"," 1   geo_accession             623 non-null    object \n"," 2   gse_id                    623 non-null    object \n"," 3   ctrl                      623 non-null    float64\n"," 4   pert                      623 non-null    float64\n"," 5   channel_count             623 non-null    int64  \n"," 6   characteristics_ch1       623 non-null    object \n"," 7   contact_address           623 non-null    object \n"," 8   contact_city              623 non-null    object \n"," 9   contact_country           623 non-null    object \n"," 10  contact_department        503 non-null    object \n"," 11  contact_email             394 non-null    object \n"," 12  contact_institute         623 non-null    object \n"," 13  contact_name              623 non-null    object \n"," 14  contact_state             449 non-null    object \n"," 15  data_processing           623 non-null    object \n"," 16  data_row_count            623 non-null    int64  \n"," 17  description               476 non-null    object \n"," 18  extract_protocol_ch1      623 non-null    object \n"," 19  growth_protocol_ch1       282 non-null    object \n"," 20  hyb_protocol              513 non-null    object \n"," 21  label_ch1                 513 non-null    object \n"," 22  label_protocol_ch1        513 non-null    object \n"," 23  last_update_date          623 non-null    object \n"," 24  molecule_ch1              623 non-null    object \n"," 25  organism_ch1              623 non-null    object \n"," 26  platform_id               623 non-null    object \n"," 27  scan_protocol             513 non-null    object \n"," 28  source_name_ch1           623 non-null    object \n"," 29  status                    623 non-null    object \n"," 30  submission_date           623 non-null    object \n"," 31  supplementary_file        513 non-null    object \n"," 32  taxid_ch1                 623 non-null    int64  \n"," 33  title                     623 non-null    object \n"," 34  treatment_protocol_ch1    298 non-null    object \n"," 35  type                      623 non-null    object \n"," 36  contact_phone             160 non-null    object \n"," 37  contact_laboratory        344 non-null    object \n"," 38  relation                  137 non-null    object \n"," 39  contact_fax               15 non-null     object \n"," 40  biomaterial_provider_ch1  14 non-null     object \n"," 41  contact_web_link          9 non-null      object \n"," 42  characteristics_ch2       26 non-null     object \n"," 43  extract_protocol_ch2      26 non-null     object \n"," 44  label_ch2                 26 non-null     object \n"," 45  label_protocol_ch2        26 non-null     object \n"," 46  molecule_ch2              26 non-null     object \n"," 47  organism_ch2              26 non-null     object \n"," 48  source_name_ch2           26 non-null     object \n"," 49  taxid_ch2                 26 non-null     float64\n"," 50  treatment_protocol_ch2    15 non-null     object \n"," 51  biomaterial_provider_ch2  4 non-null      object \n"," 52  growth_protocol_ch2       9 non-null      object \n","dtypes: float64(3), int64(4), object(46)\n","memory usage: 258.1+ KB\n"]}],"source":["train_df.info()"]},{"cell_type":"markdown","metadata":{},"source":["## Feature seletction\n","#### Finding the appropriate,useful features to achieve the best possible performance of ML algorithm"]},{"cell_type":"code","execution_count":173,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:39:27.894448Z","iopub.status.busy":"2022-03-02T08:39:27.893921Z","iopub.status.idle":"2022-03-02T08:39:27.931329Z","shell.execute_reply":"2022-03-02T08:39:27.930448Z","shell.execute_reply.started":"2022-03-02T08:39:27.894400Z"},"trusted":true},"outputs":[],"source":["'''\n","After alot of experimentation, analysis, study following 5 features gave me the best accuracy-\n","\n","[\"characteristics_ch1\",\"molecule_ch1\",\"organism_ch1\",'type',\"title\"]\n","'''\n","\n","\n","cols=[\"characteristics_ch1\",\"molecule_ch1\",\"organism_ch1\",'type',\"title\"] \n","\n","train_df.loc[:, 'feature'] = \"\"\n","# We are concatenating all the columns with a space.\n","train_df['feature'] = train_df.apply(lambda row: ' '.join([str(row[i]) for i in cols]), axis=1)\n","stop_words = set(stopwords.words('english'))\n"]},{"cell_type":"code","execution_count":174,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:39:27.933090Z","iopub.status.busy":"2022-03-02T08:39:27.932442Z","iopub.status.idle":"2022-03-02T08:39:27.940895Z","shell.execute_reply":"2022-03-02T08:39:27.940152Z","shell.execute_reply.started":"2022-03-02T08:39:27.933050Z"},"trusted":true},"outputs":[{"data":{"text/plain":["1    597\n","2     26\n","Name: channel_count, dtype: int64"]},"execution_count":174,"metadata":{},"output_type":"execute_result"}],"source":["train_df[\"channel_count\"].value_counts()"]},{"cell_type":"code","execution_count":175,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:39:27.942852Z","iopub.status.busy":"2022-03-02T08:39:27.942389Z","iopub.status.idle":"2022-03-02T08:39:27.954793Z","shell.execute_reply":"2022-03-02T08:39:27.953790Z","shell.execute_reply.started":"2022-03-02T08:39:27.942801Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0      facs sorting: CD44low/CD24high polyA RNA Homo ...\n","1      facs sorting: Unsorted polyA RNA Homo sapiens ...\n","2      facs sorting: CD44low/CD24high polyA RNA Homo ...\n","3      facs sorting: CD44high/CD24low polyA RNA Homo ...\n","4      tissue: human nasal polyp total RNA Homo sapie...\n","                             ...                        \n","618    cell line: MCF7 total RNA Homo sapiens SRA RNA...\n","619    cell line: MCF7 total RNA Homo sapiens SRA RNA...\n","620    cell line: MCF7 total RNA Homo sapiens SRA RNA...\n","621    cell line: MCF7 total RNA Homo sapiens SRA RNA...\n","622    cell line: MCF7 total RNA Homo sapiens SRA RNA...\n","Name: feature, Length: 623, dtype: object"]},"execution_count":175,"metadata":{},"output_type":"execute_result"}],"source":["train_df[\"feature\"]"]},{"cell_type":"code","execution_count":176,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:39:27.958284Z","iopub.status.busy":"2022-03-02T08:39:27.957719Z","iopub.status.idle":"2022-03-02T08:39:28.246878Z","shell.execute_reply":"2022-03-02T08:39:28.245764Z","shell.execute_reply.started":"2022-03-02T08:39:27.958236Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 623/623 [00:00<00:00, 2289.74it/s]\n"]}],"source":["def preprocess(data_df):\n","    data_df['cleaned_feature'] = ''\n","    stop_words = set(stopwords.words('english'))\n","    wordnet_lemm = WordNetLemmatizer()\n","    for index, row in tqdm(data_df.iterrows(), total=data_df.shape[0]):\n","        sample = row['feature']\n","        \n","        pre_txt = re.sub(r\"[^a-zA-Z0-9- ]\", \" \", sample)\n","        pre_txt = pre_txt.lower()\n","        sample_words = [wordnet_lemm.lemmatize(w) for w in pre_txt.split() if w not in stop_words and len(w)>1]\n","        pre_proc_ver = ' '.join(sample_words)\n","        data_df.loc[index, 'cleaned_feature'] = pre_proc_ver\n","    return data_df\n","        \n","# Cleaned Training set\n","cleaned_train_df = preprocess(train_df.copy())\n"]},{"cell_type":"code","execution_count":177,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:39:28.249167Z","iopub.status.busy":"2022-03-02T08:39:28.248792Z","iopub.status.idle":"2022-03-02T08:39:28.255448Z","shell.execute_reply":"2022-03-02T08:39:28.254587Z","shell.execute_reply.started":"2022-03-02T08:39:28.249120Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'facs sorting cd44low cd24high polya rna homo sapiens sra rep1 cd44low dox'"]},"execution_count":177,"metadata":{},"output_type":"execute_result"}],"source":["cleaned_train_df[\"cleaned_feature\"][0]"]},{"cell_type":"markdown","metadata":{},"source":["## Feature extraction and encoding "]},{"cell_type":"code","execution_count":178,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:39:28.257080Z","iopub.status.busy":"2022-03-02T08:39:28.256806Z","iopub.status.idle":"2022-03-02T08:39:28.270916Z","shell.execute_reply":"2022-03-02T08:39:28.270000Z","shell.execute_reply.started":"2022-03-02T08:39:28.257044Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'\\nI had extensive experimentation between TFIDF and bag-of-words.  \\nBag of words (countvectorizer) approach gave me the best score. May be because our input features had \\nlarge vocabulary of different/unique words that was resulting into correct predictions as TFIDF works on the\\nprinciple of frequency of words, because of the uniqueness of words the frequency was going to be 1 for most of the cases.\\n'"]},"execution_count":178,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","I had extensive experimentation between TFIDF and bag-of-words.  \n","Bag of words (countvectorizer) approach gave me the best score. May be because our input features had \n","large vocabulary of different/unique words that was resulting into correct predictions as TFIDF works on the\n","principle of frequency of words, because of the uniqueness of words the frequency was going to be 1 for most of the cases.\n","'''\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### a) Extracting features from unlabelled data"]},{"cell_type":"code","execution_count":179,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:39:28.272553Z","iopub.status.busy":"2022-03-02T08:39:28.272306Z","iopub.status.idle":"2022-03-02T08:39:28.281758Z","shell.execute_reply":"2022-03-02T08:39:28.281061Z","shell.execute_reply.started":"2022-03-02T08:39:28.272520Z"},"trusted":true},"outputs":[],"source":["vect = CountVectorizer(analyzer=\"word\", preprocessor=None, stop_words=stop_words, max_features=15000)"]},{"cell_type":"code","execution_count":180,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:39:28.284575Z","iopub.status.busy":"2022-03-02T08:39:28.284189Z","iopub.status.idle":"2022-03-02T08:39:42.291877Z","shell.execute_reply":"2022-03-02T08:39:42.290900Z","shell.execute_reply.started":"2022-03-02T08:39:28.284525Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 15000/15000 [00:11<00:00, 1306.20it/s]\n"]}],"source":["\n","unlabelled_df=pd.read_csv(\"../input/mlrw-biomedicalhackathon/unlabelled_train_data.csv\").sample(15000,random_state=42)\n","\n","unlabelled_df['feature'] = unlabelled_df.apply(lambda row: ' '.join([str(row[i]) for i in cols]), axis=1)\n","cleaned_unlabelled_df = preprocess(unlabelled_df.copy())\n","\n","vect=vect1.fit(cleaned_unlabelled_df['cleaned_feature'])\n","X_unlabelled=vect.transform(cleaned_unlabelled_df['cleaned_feature'])\n","X_unlabelled=X_unlabelled.toarray()"]},{"cell_type":"markdown","metadata":{},"source":["#### b) Extracting features from labelled data"]},{"cell_type":"code","execution_count":181,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:39:42.293471Z","iopub.status.busy":"2022-03-02T08:39:42.293217Z","iopub.status.idle":"2022-03-02T08:39:42.358382Z","shell.execute_reply":"2022-03-02T08:39:42.357342Z","shell.execute_reply.started":"2022-03-02T08:39:42.293439Z"},"trusted":true},"outputs":[],"source":["\n","X_train_vect = vect.transform(cleaned_train_df['cleaned_feature'])\n","y_train = cleaned_train_df['ctrl'].astype(int).tolist()\n","\n","X = vect1.transform(cleaned_train_df['cleaned_feature']).toarray()\n","Y = cleaned_train_df['ctrl'].astype(int).tolist()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Spliting into training and testing data (labelled)"]},{"cell_type":"code","execution_count":182,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:39:42.360366Z","iopub.status.busy":"2022-03-02T08:39:42.359944Z","iopub.status.idle":"2022-03-02T08:39:42.393268Z","shell.execute_reply":"2022-03-02T08:39:42.392476Z","shell.execute_reply.started":"2022-03-02T08:39:42.360317Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train_vect,X_valid_vect,y_train,y_valid =train_test_split(X_train_vect,y_train,test_size=0.15,\\\n","                                                            stratify=y_train,random_state=42)\n","X_train_vect=X_train_vect.toarray()\n","X_valid_vect=X_valid_vect.toarray()\n","# X_train_vect"]},{"cell_type":"code","execution_count":183,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:39:42.394602Z","iopub.status.busy":"2022-03-02T08:39:42.394389Z","iopub.status.idle":"2022-03-02T08:39:42.400792Z","shell.execute_reply":"2022-03-02T08:39:42.400006Z","shell.execute_reply.started":"2022-03-02T08:39:42.394575Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(529, 9566)"]},"execution_count":183,"metadata":{},"output_type":"execute_result"}],"source":["X_train_vect.shape"]},{"cell_type":"code","execution_count":184,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:39:42.401951Z","iopub.status.busy":"2022-03-02T08:39:42.401746Z","iopub.status.idle":"2022-03-02T08:39:42.414282Z","shell.execute_reply":"2022-03-02T08:39:42.413565Z","shell.execute_reply.started":"2022-03-02T08:39:42.401925Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'\\nI wrote the following code just to see how different types of classification models are performing on \\ntiny labelled data (600 rows).\\n'"]},"execution_count":184,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","I wrote the following code just to see how different types of classification models are performing on \n","tiny labelled data (600 rows).\n","'''"]},{"cell_type":"code","execution_count":185,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:39:42.416438Z","iopub.status.busy":"2022-03-02T08:39:42.415688Z","iopub.status.idle":"2022-03-02T08:39:42.426837Z","shell.execute_reply":"2022-03-02T08:39:42.426060Z","shell.execute_reply.started":"2022-03-02T08:39:42.416402Z"},"trusted":true},"outputs":[],"source":["# from sklearn.linear_model import LogisticRegression,LinearRegression\n","# from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier\n","# from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n","# from sklearn.tree import ExtraTreeClassifier\n","\n","# from xgboost import XGBClassifier\n","# import lightgbm as lgb\n","# from sklearn.neural_network import MLPClassifier\n","\n","# from sklearn.ensemble.bagging import BaggingClassifier\n","# from sklearn.tree import DecisionTreeClassifier\n","# from sklearn.naive_bayes import GaussianNB,BernoulliNB\n","# from sklearn.svm import LinearSVC\n","# from sklearn.metrics import f1_score,roc_auc_score\n","# models={\n","#         \"Logi\":LogisticRegression(max_iter=1000),\n","#         \"SVM\":LinearSVC(random_state=0),\n","#         \"Naive bayes\":BernoulliNB(),\n","#         \"RandomForest\":RandomForestClassifier(n_estimators=100,random_state=0),\n","#         \"Decision Tree\": DecisionTreeClassifier(),\n","#         \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=0),\n","#         \"Extra tree\":ExtraTreeClassifier(),\n","#         \"gradient_boosting\":GradientBoostingClassifier(),\n","#         \"xgb\":XGBClassifier(n_estimators=200, learning_rate=0.05),\n","#         \"lgbm\":lgb.LGBMClassifier(learning_rate=0.01,num_iterations=10000,n_estimators=100),\n","#         \"mlp\":MLPClassifier(random_state=1, max_iter=500)\n","\n","#         }\n","# nam,scores,f1,a=[],[],[],[]\n","# for name,model in zip(list(models.keys()),list(models.values())):\n","#     model.fit(X_train_vect, y_train)\n","#     y_pred=model.predict(X_valid_vect)\n","#     nam.append(name)\n","#     scores.append(model.score(X_valid_vect,y_valid))\n","#     f1.append(f1_score(y_valid,y_pred))\n","#     a.append(roc_auc_score(y_valid,y_pred))\n","\n","# expt=pd.DataFrame({\"Model\":nam,\"Accuracy\":scores,\"F1_Score\":f1,\"auc\":a})\n","# expt\n"]},{"cell_type":"markdown","metadata":{},"source":["# Weekly supervised learning (Data Curation)"]},{"cell_type":"code","execution_count":186,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:39:42.430900Z","iopub.status.busy":"2022-03-02T08:39:42.430464Z","iopub.status.idle":"2022-03-02T08:39:42.445440Z","shell.execute_reply":"2022-03-02T08:39:42.444416Z","shell.execute_reply.started":"2022-03-02T08:39:42.430850Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(15000, 9566)"]},"execution_count":186,"metadata":{},"output_type":"execute_result"}],"source":["X_unlabelled.shape"]},{"cell_type":"markdown","metadata":{},"source":["### Generating Labels \n","#### Training (voting classifier) model on labelled data and using that to generate labels for Unlabelled-data"]},{"cell_type":"code","execution_count":187,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:39:42.447468Z","iopub.status.busy":"2022-03-02T08:39:42.446857Z","iopub.status.idle":"2022-03-02T08:41:40.586592Z","shell.execute_reply":"2022-03-02T08:41:40.585441Z","shell.execute_reply.started":"2022-03-02T08:39:42.447432Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n","  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["[08:40:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model</th>\n","      <th>Accuracy</th>\n","      <th>F1_Score</th>\n","      <th>auc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Naive bayes</td>\n","      <td>0.851064</td>\n","      <td>0.840909</td>\n","      <td>0.850091</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>RandomForest</td>\n","      <td>0.893617</td>\n","      <td>0.893617</td>\n","      <td>0.894022</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Decision Tree</td>\n","      <td>0.946809</td>\n","      <td>0.948454</td>\n","      <td>0.947917</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>AdaBoost</td>\n","      <td>0.936170</td>\n","      <td>0.938776</td>\n","      <td>0.937500</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>gradient_boosting</td>\n","      <td>0.904255</td>\n","      <td>0.901099</td>\n","      <td>0.903986</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>xgb</td>\n","      <td>0.861702</td>\n","      <td>0.850575</td>\n","      <td>0.860507</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>mlp</td>\n","      <td>0.914894</td>\n","      <td>0.920000</td>\n","      <td>0.916667</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               Model  Accuracy  F1_Score       auc\n","0        Naive bayes  0.851064  0.840909  0.850091\n","1       RandomForest  0.893617  0.893617  0.894022\n","2      Decision Tree  0.946809  0.948454  0.947917\n","3           AdaBoost  0.936170  0.938776  0.937500\n","4  gradient_boosting  0.904255  0.901099  0.903986\n","5                xgb  0.861702  0.850575  0.860507\n","6                mlp  0.914894  0.920000  0.916667"]},"execution_count":187,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","Here I have used the output of 7 ML models(voting classifier) and generated the labels \n","'''\n","models={\n","        \"Naive bayes\":BernoulliNB(),\n","        \"RandomForest\":RandomForestClassifier(n_estimators=100,random_state=0),\n","        \"Decision Tree\": DecisionTreeClassifier(),\n","        \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=0),\n","        \"gradient_boosting\":GradientBoostingClassifier(),\n","        \"xgb\":XGBClassifier(n_estimators=100, learning_rate=0.05),\n","        \"mlp\":MLPClassifier(random_state=1, max_iter=500)\n","        }\n","\n","p=[]\n","nam,scores,f1,a=[],[],[],[]\n","\n","for name,model in zip(list(models.keys()),list(models.values())):\n","    model.fit(X_train_vect, y_train)\n","    y_pred=model.predict_proba(X_unlabelled)\n","    p.append(y_pred)\n","    y_pred=model.predict(X_valid_vect)\n","    nam.append(name)\n","    scores.append(model.score(X_valid_vect,y_valid))\n","    f1.append(f1_score(y_valid,y_pred))\n","    a.append(roc_auc_score(y_valid,y_pred))\n","    \n","expt=pd.DataFrame({\"Model\":nam,\"Accuracy\":scores,\"F1_Score\":f1,\"auc\":a})\n","expt\n","    "]},{"cell_type":"code","execution_count":188,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:41:40.589766Z","iopub.status.busy":"2022-03-02T08:41:40.588961Z","iopub.status.idle":"2022-03-02T08:41:40.600687Z","shell.execute_reply":"2022-03-02T08:41:40.599694Z","shell.execute_reply.started":"2022-03-02T08:41:40.589705Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([1, 0, 1, 1, 1])"]},"execution_count":188,"metadata":{},"output_type":"execute_result"}],"source":["pred=np.array(p)\n","pred=np.argmax(np.sum(pred,axis=0)/5,axis=1)\n","pred[:5]"]},{"cell_type":"code","execution_count":189,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:41:40.603643Z","iopub.status.busy":"2022-03-02T08:41:40.602849Z","iopub.status.idle":"2022-03-02T08:41:40.613468Z","shell.execute_reply":"2022-03-02T08:41:40.612367Z","shell.execute_reply.started":"2022-03-02T08:41:40.603584Z"},"trusted":true},"outputs":[{"data":{"text/plain":["15000"]},"execution_count":189,"metadata":{},"output_type":"execute_result"}],"source":["y_predictions=pred\n","len(y_predictions)"]},{"cell_type":"code","execution_count":190,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:41:40.616648Z","iopub.status.busy":"2022-03-02T08:41:40.615835Z","iopub.status.idle":"2022-03-02T08:41:40.629926Z","shell.execute_reply":"2022-03-02T08:41:40.628687Z","shell.execute_reply.started":"2022-03-02T08:41:40.616588Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0    11217\n","1     3783\n","dtype: int64"]},"execution_count":190,"metadata":{},"output_type":"execute_result"}],"source":["pd.Series(y_predictions).value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["### Concatenating the labelled and unlabelled data after generating the labels \n","#### So now we have around 15,000 datapoints to train the final classification model"]},{"cell_type":"code","execution_count":191,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:41:40.633445Z","iopub.status.busy":"2022-03-02T08:41:40.632340Z","iopub.status.idle":"2022-03-02T08:41:41.066050Z","shell.execute_reply":"2022-03-02T08:41:41.065065Z","shell.execute_reply.started":"2022-03-02T08:41:40.633386Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["X: (15623, 9566), Y: (15623,)\n"]}],"source":["X=np.concatenate([X,X_unlabelled],axis=0)\n","Y=np.concatenate([Y,y_predictions],axis=0)\n","\n","print(f\"X: {X.shape}, Y: {Y.shape}\")"]},{"cell_type":"code","execution_count":192,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:41:41.067519Z","iopub.status.busy":"2022-03-02T08:41:41.067296Z","iopub.status.idle":"2022-03-02T08:41:41.440411Z","shell.execute_reply":"2022-03-02T08:41:41.439524Z","shell.execute_reply.started":"2022-03-02T08:41:41.067491Z"},"trusted":true},"outputs":[],"source":["X_train_vect,X_valid_vect,y_train,y_valid =train_test_split(X,Y,test_size=0.20,\\\n","                                                            stratify=Y,random_state=42)"]},{"cell_type":"code","execution_count":193,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:41:41.442357Z","iopub.status.busy":"2022-03-02T08:41:41.441778Z","iopub.status.idle":"2022-03-02T08:41:41.446752Z","shell.execute_reply":"2022-03-02T08:41:41.446147Z","shell.execute_reply.started":"2022-03-02T08:41:41.442324Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(12498, 9566)"]},"execution_count":193,"metadata":{},"output_type":"execute_result"}],"source":["X_train_vect.shape"]},{"cell_type":"markdown","metadata":{},"source":["## Training the final classification model\n","#### Experimentating with different types of models"]},{"cell_type":"code","execution_count":194,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:41:41.448327Z","iopub.status.busy":"2022-03-02T08:41:41.448078Z","iopub.status.idle":"2022-03-02T08:46:59.005775Z","shell.execute_reply":"2022-03-02T08:46:59.005170Z","shell.execute_reply.started":"2022-03-02T08:41:41.448298Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model</th>\n","      <th>Accuracy</th>\n","      <th>F1_Score</th>\n","      <th>auc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Logi</td>\n","      <td>0.98240</td>\n","      <td>0.965646</td>\n","      <td>0.970327</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Naive bayes</td>\n","      <td>0.93376</td>\n","      <td>0.877151</td>\n","      <td>0.923970</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>SVM</td>\n","      <td>0.98656</td>\n","      <td>0.974265</td>\n","      <td>0.981823</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Decision Tree</td>\n","      <td>0.98464</td>\n","      <td>0.970874</td>\n","      <td>0.982496</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>AdaBoost</td>\n","      <td>0.97344</td>\n","      <td>0.947897</td>\n","      <td>0.957157</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           Model  Accuracy  F1_Score       auc\n","0           Logi   0.98240  0.965646  0.970327\n","1    Naive bayes   0.93376  0.877151  0.923970\n","2            SVM   0.98656  0.974265  0.981823\n","3  Decision Tree   0.98464  0.970874  0.982496\n","4       AdaBoost   0.97344  0.947897  0.957157"]},"execution_count":194,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","As ensemble models were taking too much time to train. I have commented them. You can uncomment them if you want to.\n","But trust me all powerfull models like randomforest, xgb, lgbm are tending to overfit and performing terrible\n","\n","'''\n","from sklearn.linear_model import LogisticRegression,LinearRegression\n","from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import GaussianNB,BernoulliNB\n","from sklearn.svm import LinearSVC,SVC\n","from sklearn.metrics import f1_score,roc_auc_score\n","models={\n","        \"Logi\":LogisticRegression(max_iter=1000),\n","        \"Naive bayes\":BernoulliNB(),\n","         \"SVM\":LinearSVC(random_state=0),\n","        \"Decision Tree\": DecisionTreeClassifier(),\n","        \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=0),\n","#         \"gradient_boosting\":GradientBoostingClassifier(),\n","#         \"xgb\":XGBClassifier(n_estimators=100, learning_rate=0.05),\n","#         \"gradient_boosting\":GradientBoostingClassifier(),\n","        }\n","nam,scores,f1,a=[],[],[],[]\n","for name,model in zip(list(models.keys()),list(models.values())):\n","    model.fit(X_train_vect, y_train)\n","    y_pred=model.predict(X_valid_vect)\n","    nam.append(name)\n","    scores.append(model.score(X_valid_vect,y_valid))\n","    f1.append(f1_score(y_valid,y_pred))\n","    a.append(roc_auc_score(y_valid,y_pred))\n","\n","expt=pd.DataFrame({\"Model\":nam,\"Accuracy\":scores,\"F1_Score\":f1,\"auc\":a})\n","expt\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Using the best possible model on test_data for predictions"]},{"cell_type":"code","execution_count":195,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:46:59.007301Z","iopub.status.busy":"2022-03-02T08:46:59.006908Z","iopub.status.idle":"2022-03-02T08:47:12.851831Z","shell.execute_reply":"2022-03-02T08:47:12.850687Z","shell.execute_reply.started":"2022-03-02T08:46:59.007265Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Classification report for ctrl (1 - ctrl, 0 - pert)\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.98      1.00      0.99      2307\n","           1       0.99      0.94      0.97       818\n","\n","    accuracy                           0.98      3125\n","   macro avg       0.98      0.97      0.98      3125\n","weighted avg       0.98      0.98      0.98      3125\n","\n"]}],"source":["model=LogisticRegression(max_iter=1000)\n","# model=LinearSVC(random_state=0)\n","\n","model.fit(X_train_vect, y_train)\n","print(\"Classification report for ctrl (1 - ctrl, 0 - pert)\\n\")\n","y_predictions = model.predict(X_valid_vect)\n","print(classification_report(y_valid, y_predictions))\n","\n"]},{"cell_type":"code","execution_count":196,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:47:12.853624Z","iopub.status.busy":"2022-03-02T08:47:12.853184Z","iopub.status.idle":"2022-03-02T08:47:17.022346Z","shell.execute_reply":"2022-03-02T08:47:17.021387Z","shell.execute_reply.started":"2022-03-02T08:47:12.853578Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 6070/6070 [00:03<00:00, 1844.66it/s]\n"]}],"source":["\n","test_df = pd.read_csv('../input/mlrw-biomedicalhackathon/data_only_test.csv')\n","\n","test_df.loc[:, 'feature'] = \"\"\n","test_df['feature'] = test_df.apply(lambda row: ' '.join([str(row[i]) for i in cols]), axis=1)\n","\n","cleaned_test_df = preprocess(test_df.copy())\n","X_test_vect = final_vect.transform(cleaned_test_df['cleaned_feature'].tolist())\n","X_test_vect=X_test_vect.toarray()"]},{"cell_type":"code","execution_count":197,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T08:47:17.024463Z","iopub.status.busy":"2022-03-02T08:47:17.023939Z","iopub.status.idle":"2022-03-02T08:47:17.312959Z","shell.execute_reply":"2022-03-02T08:47:17.311669Z","shell.execute_reply.started":"2022-03-02T08:47:17.024425Z"},"trusted":true},"outputs":[],"source":["\n","test_predictions = model.predict(X_test_vect)\n","cleaned_test_df['ctrl'] = test_predictions\n","\n","cleaned_test_df = cleaned_test_df[['geo_accession', 'ctrl']]\n","\n","# make sure its a float!\n","cleaned_test_df['ctrl'] = cleaned_test_df['ctrl'].astype(np.float64)\n","\n","cleaned_test_df.to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":200,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T09:11:48.792385Z","iopub.status.busy":"2022-03-02T09:11:48.792046Z","iopub.status.idle":"2022-03-02T09:11:48.799339Z","shell.execute_reply":"2022-03-02T09:11:48.798349Z","shell.execute_reply.started":"2022-03-02T09:11:48.792350Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'\\nBest Submission-\\n77.90  15k voting+logi countvect cols=[\"characteristics_ch1\",\"molecule_ch1\",\"organism_ch1\",\\'type\\',\"title\"] \\n\\nThe submission that gave me the best rank on final private LB had score of 77.90 on public LB. For the submission\\nI used around 15,000 samples from unalabelled data. Trained a voting classifier on labelled dataset. And\\nused that classifier to predict/generate the labels of unlablled data. And then finally trained a logistic-regression\\nmodel on the 15k datapoints, \\n\\n'"]},"execution_count":200,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","Best Submission-\n","77.90  15k voting+logi countvect cols=[\"characteristics_ch1\",\"molecule_ch1\",\"organism_ch1\",'type',\"title\"] \n","\n","The submission that gave me the best rank on final private LB had score of 77.90 on public LB. For the submission\n","I used around 15,000 samples from unalabelled data. Trained a voting classifier on labelled dataset. And\n","used that classifier to predict/generate the labels of unlablled data. And then finally trained a logistic-regression\n","model on the 15k datapoints, \n","\n","'''"]},{"cell_type":"markdown","metadata":{},"source":["### Experimentation LOG\n","\n","##### Format-\n","##### LBScore----Datapoints_used----label_Generator_Model+Final_classification_model----Preprocessing----Cols_used"]},{"cell_type":"code","execution_count":199,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T09:10:11.065438Z","iopub.status.busy":"2022-03-02T09:10:11.064352Z","iopub.status.idle":"2022-03-02T09:10:11.073936Z","shell.execute_reply":"2022-03-02T09:10:11.072974Z","shell.execute_reply.started":"2022-03-02T09:10:11.065396Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'\\nExperimentation Results-->\\n\\n1] 71.136 10k DT+SVM without regex cols = [\"channel_count\",\"characteristics_ch1\",\"label_ch1\",\"molecule_ch1\",\"organism_ch1\",\\'source_name_ch1\\',\\'type\\',\"title\"] \\n\\n2] 75.37 10k adabost+svm prerpocessed regex Countvect cols = [\"characteristics_ch1\",\"label_ch1\",\"molecule_ch1\",\"organism_ch1\",\\'type\\',\"title\"] \\n\\n3]76.47  15k adabost+svm prerpocessed regex TFIDF  cols = [\"characteristics_ch1\",\"label_ch1\",\"molecule_ch1\",\"organism_ch1\",\\'type\\',\"title\"] \\n\\n4]75.584  20k DT+svm prerpocessed regex TFIDF cols = [\"characteristics_ch1\",\"label_ch1\",\"molecule_ch1\",\"organism_ch1\",\\'type\\',\"title\"] \\n\\n5]75.881  20k adabost+svm CountVect cols = [\"characteristics_ch1\",\"label_ch1\",\"molecule_ch1\",\"organism_ch1\",\\'type\\',\"title\"]\\n\\n6] 75.189 8k DT+svm CountVect cols=[\"channel_count\",\"characteristics_ch1\",\"molecule_ch1\",\"organism_ch1\",\\'source_name_ch1\\',\\'type\\',\"title\"] \\n\\n7]76.67 10k  voting + SVM  cols=[\"characteristics_ch1\",\"label_ch1\",\"molecule_ch1\",\"organism_ch1\",\\'type\\',\"title\"] \\n\\n8]77.429 15k  voting + SVM  cols=[\"characteristics_ch1\",\"molecule_ch1\",\"organism_ch1\",\\'type\\',\"title\"] \\n\\n9]77.90  15k voting+logi cols=[\"characteristics_ch1\",\"molecule_ch1\",\"organism_ch1\",\\'type\\',\"title\"] \\n'"]},"execution_count":199,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","Experimentation Results-->\n","\n","1] 71.136 10k DT+SVM without regex cols = [\"channel_count\",\"characteristics_ch1\",\"label_ch1\",\"molecule_ch1\",\"organism_ch1\",'source_name_ch1','type',\"title\"] \n","\n","2] 75.37 10k adabost+svm prerpocessed regex Countvect cols = [\"characteristics_ch1\",\"label_ch1\",\"molecule_ch1\",\"organism_ch1\",'type',\"title\"] \n","\n","3]76.47  15k adabost+svm prerpocessed regex TFIDF  cols = [\"characteristics_ch1\",\"label_ch1\",\"molecule_ch1\",\"organism_ch1\",'type',\"title\"] \n","\n","4]75.584  20k DT+svm prerpocessed regex TFIDF cols = [\"characteristics_ch1\",\"label_ch1\",\"molecule_ch1\",\"organism_ch1\",'type',\"title\"] \n","\n","5]75.881  20k adabost+svm CountVect cols = [\"characteristics_ch1\",\"label_ch1\",\"molecule_ch1\",\"organism_ch1\",'type',\"title\"]\n","\n","6] 75.189 8k DT+svm CountVect cols=[\"channel_count\",\"characteristics_ch1\",\"molecule_ch1\",\"organism_ch1\",'source_name_ch1','type',\"title\"] \n","\n","7]76.67 10k  voting + SVM  cols=[\"characteristics_ch1\",\"label_ch1\",\"molecule_ch1\",\"organism_ch1\",'type',\"title\"] \n","\n","8]77.429 15k  voting + SVM  cols=[\"characteristics_ch1\",\"molecule_ch1\",\"organism_ch1\",'type',\"title\"] \n","\n","9]77.90  15k voting+logi cols=[\"characteristics_ch1\",\"molecule_ch1\",\"organism_ch1\",'type',\"title\"] \n","'''"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
